{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Full workflow\n",
    "\n",
    "This notebook demonstrates the full workflow. The steps include:\n",
    "1. Create a virtual library, we will do this exemplary for monosubstituted thiophenes\n",
    "2. Download the models from Zenodo\n",
    "3. Predict the triplet energy and the spin density using the models\n",
    "4. Use an automated analysis to find the most promising candidates\n",
    "5. Visualize the results using an interactive analysis\n",
    "\n",
    "In order to run this notebook, you need to have cloned this repository and installed the required packages"
   ],
   "id": "40ecb5f287dd8dc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create the virtual space\n",
    "First we want to create the virtual library. For this you need to give the name of the directory where the project is located at."
   ],
   "id": "304a67ab1c9c4a97"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "home_dir = r'\\YOUR_PATH\\EnT_Substrate_Mapping\\\\' #Change this to the directory your project is located at.\n",
    "sys.path.append(home_dir) #Add the directory to your pythonpath, so that the functions from Combinatorial.help_to_combine can be imported.\n",
    "from CombinatorialSpace.help_to_combine import get_functional_groups, create_space\n",
    "\n",
    "path_to_fg_list = home_dir + r'/CombinatorialSpace/functional_groups.txt' #This file contains the functional groups that can be added to the core. You can add your own functional groups to this file.\n",
    "functional_groups = get_functional_groups(path_to_fg_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we generate a virtual library of monosubstituted thiophenes. For other heterocycles the corresponding smiles needs to be given. The number for num_iterations determines how many times the functional groups are added to the core. The keep_all parameter determines whether all possible combinations of functional groups are kept or only the first one. To get the space of disubstituted molecules set num_iterations=2 and for the triubstituted to num_iterations=3.",
   "id": "638c7ff7901495db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "smiles = 'C1=CC=CS1'\n",
    "ring_class = 'thiophene'\n",
    "substitution = 'monosubstituted'\n",
    "\n",
    "virtual_space = create_space(functional_groups, smiles, smiles, num_iterations=1, keep_all=True)\n",
    "df = pd.DataFrame(data={'smiles': virtual_space})\n",
    "df.to_csv(home_dir + f'/Data/virtual_libraries/{ring_class}_{substitution}.csv', index=False, columns=['smiles'])"
   ],
   "id": "eddcce8601f616e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we've created the chemical space we want to predict the triplet energy and the spin density. For this we first download the models from Zenodo. Depending on your internet connection this might take a few minutes.",
   "id": "81d69fd6602de75c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "zenodo_file_url = 'https://zenodo.org/records/10391170/files/spin_population.pt?download=1'\n",
    "output_filename = home_dir + 'Models/spin_population/spin_population.pt'\n",
    "response = requests.get(zenodo_file_url, stream=True)\n",
    "\n",
    "# Check if the download was successful\n",
    "if response.status_code == 200:\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"Downloaded successfully as {output_filename}\")\n",
    "else:\n",
    "    print(f\"Failed to download. Status code: {response.status_code}\")\n",
    "\n",
    "zenodo_file_url = 'https://zenodo.org/records/10391170/files/triplet_energy.pt?download=1'\n",
    "output_filename = home_dir + 'Models/triplet_energy/triplet_energy.pt'\n",
    "response = requests.get(zenodo_file_url, stream=True)\n",
    "\n",
    "# Check if the download was successful\n",
    "if response.status_code == 200:\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"Downloaded successfully as {output_filename}\")\n",
    "else:\n",
    "    print(f\"Failed to download. Status code: {response.status_code}\")"
   ],
   "id": "6e33145660c25af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have our virtual space and the models we can use the models for predictions. Let's start with the triplet energy:",
   "id": "e0466dbb72ea8683"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import contextlib\n",
    "import chemprop\n",
    "\n",
    "path_to_model = home_dir + 'Models/triplet_energy/triplet_energy.pt'\n",
    "path_to_molecules = home_dir + f'Data/virtual_libraries/{ring_class}_{substitution}.csv'\n",
    "path_for_predictions = home_dir + f'Data/et_predictions/predictions_{ring_class}_{substitution}.csv'\n",
    "\n",
    "arguments = [\n",
    "    '--test_path', path_to_molecules,\n",
    "    '--preds_path', path_for_predictions,\n",
    "    '--num_workers', '1',\n",
    "    '--checkpoint_path', path_to_model,\n",
    "    '--features_generator', 'rdkit_2d_normalized',\n",
    "    '--no_features_scaling'\n",
    "]\n",
    "\n",
    "with contextlib.redirect_stdout(None):\n",
    "    args = chemprop.args.PredictArgs().parse_args(arguments)\n",
    "    preds = chemprop.train.make_predictions(args=args)"
   ],
   "id": "42c1e4de186e6c50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's continue with the spin population.",
   "id": "de1f600c1648f847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from EasyChemML.DataImport.DataImporter import DataImporter\n",
    "from EasyChemML.DataImport.Module.CSV import CSV\n",
    "from EasyChemML.Encoder import BertTokenizer\n",
    "from EasyChemML.Environment import Environment\n",
    "from EasyChemML.Encoder.impl_Tokenizer.SmilesTokenizer_SchwallerEtAll import SmilesTokenzier\n",
    "from EasyChemML.Model.impl_Pytorch.Models.BERT.FP2MOL_BERT_Trans import FP2MOL_Bert\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "file_path = home_dir + f'Data/virtual_libraries/{ring_class}_{substitution}.csv'\n",
    "path_to_model = home_dir + 'Models/spin_population/spin_population.pt'\n",
    "path_for_predictions = home_dir + f'Data/sp_predictions/predictions_{ring_class}_{substitution}.csv'\n",
    "\n",
    "# ----------------------------------- Data Preprocessing -----------------------\n",
    "device = \"cpu\" # Change this to \"cpu\" if you don't have a GPU available.\n",
    "\n",
    "###Do not change the following parameters unless you train a new model with different parameters\n",
    "d_model = 512\n",
    "heads = 4\n",
    "N = 8\n",
    "src_vocab_size =106\n",
    "trg_vocab_size = 136\n",
    "dropout = 0.1\n",
    "max_seq_len = 100\n",
    "###\n",
    "\n",
    "# Load the model\n",
    "model_object = FP2MOL_Bert(src_vocab_size, trg_vocab_size, N, heads, d_model, dropout, max_seq_len, device)\n",
    "model_object.load_model_parameters(path_to_model)\n",
    "\n",
    "env = Environment(WORKING_path_addRelativ='Output')\n",
    "\n",
    "dataloader = {'data': CSV(file_path, columns=['smiles'])}\n",
    "di = DataImporter(env)\n",
    "bp = di.load_data_InNewBatchPartition(dataloader)\n",
    "\n",
    "val_data_size = len(bp['data'])\n",
    "\n",
    "print('Start BertTokenizer')\n",
    "tokenizer = BertTokenizer()\n",
    "tokenizer.convert(datatable=bp['data'], columns=['smiles'], n_jobs=4)\n",
    "\n",
    "# ----------------------------------- Evaluation -----------------------\n",
    "\n",
    "batch_size = 5\n",
    "decoding_strategy = 'greedy'\n",
    "\n",
    "s = SmilesTokenzier()\n",
    "final_df=pd.DataFrame(data=None)\n",
    "\n",
    "start_mol = 0\n",
    "iter_steps = 0\n",
    "\n",
    "start = time.time()\n",
    "temp = start\n",
    "\n",
    "iteration_per_chunk = int(val_data_size / batch_size)\n",
    "\n",
    "for verbose, i in enumerate(range(int(iteration_per_chunk)+1)):\n",
    "\n",
    "    end_mol = start_mol + batch_size\n",
    "\n",
    "    true_smiSD = bp['data'][start_mol:end_mol]['smiles_ids']\n",
    "    input_smi = bp['data'][start_mol:end_mol]['smiles_ids']\n",
    "    input_smi = input_smi[:, 0:100]\n",
    "\n",
    "    true_smi = bp['data'][start_mol:end_mol]['smiles']\n",
    "\n",
    "    true_smiSD = torch.LongTensor(true_smiSD).to(torch.device(device))\n",
    "    input_smi = torch.LongTensor(input_smi).to(torch.device(device))\n",
    "    model_eval = model_object.fit_eval(input_smi, true_smiSD, method=decoding_strategy)\n",
    "    outputs = model_eval.outputs\n",
    "    start_mol = end_mol\n",
    "    iter_steps += 1\n",
    "    finished = False\n",
    "\n",
    "    if decoding_strategy == 'beam_search':\n",
    "        num_beams = 5\n",
    "    else:\n",
    "        num_beams = 1\n",
    "\n",
    "    for example in range(batch_size):\n",
    "        for beam in range(num_beams):\n",
    "            counter = (example * num_beams) + beam\n",
    "            pred_smi, pred_SD_string, DensityArray = s.getSmilesfromoutputwithSD(outputs[counter])\n",
    "\n",
    "            df_example = pd.DataFrame({})\n",
    "            df_example['Predicted_smiles'] = [pred_smi]\n",
    "            df_example['Predicted_SD_string'] = [pred_SD_string]\n",
    "            df_example['DensityArray'] = [DensityArray]\n",
    "            df_example['True_smiles'] = [true_smi[example].decode(\"utf-8\")]\n",
    "\n",
    "            if pred_smi == str(true_smi[example].decode(\"utf-8\")):\n",
    "                df_example['Reconstrunction_error'] = [0]\n",
    "            else:\n",
    "                df_example['Reconstrunction_error'] = [1]\n",
    "\n",
    "            final_df = pd.concat([final_df, df_example])\n",
    "            if ((i*batch_size) + example+1) == val_data_size:\n",
    "                finished = True\n",
    "                break\n",
    "\n",
    "        if finished:\n",
    "            break\n",
    "\n",
    "    print('----------------------------------')\n",
    "    print(f'Done with {verbose+1} / {iteration_per_chunk} chunks')\n",
    "\n",
    "    final_df.to_csv(path_for_predictions, index=False)"
   ],
   "id": "cf63c03361a23ad5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we generate the images for the spin density maps. For this we use the RDKit library. The images are saved in the folder Data/images.",
   "id": "d778f91e63b560c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "type_of_image = 'spin_population' # Options are structure or 'spin_population'\n",
    "                            # Caveat of spin_population is that the memory for the generation for the images can overflow and crash the script.\n",
    "                            # In this case the script needs to be run in smaller batches.\n",
    "\n",
    "data_path = home_dir + f'/Data/virtual_libraries/{ring_class}_{substitution}.csv'\n",
    "picFolder_path = home_dir + f'/Data/images/{ring_class}_{substitution}'\n",
    "if not os.path.exists(picFolder_path):\n",
    "    os.makedirs(picFolder_path)\n",
    "#Only required for spin_density images:\n",
    "sp_path = home_dir + f'/Data/sp_predictions/predictions_{ring_class}_{substitution}.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "if not os.path.exists(picFolder_path):\n",
    "    os.mkdir(picFolder_path)\n",
    "\n",
    "if type_of_image == 'structure':\n",
    "    for smi in df.smiles.to_list():\n",
    "        d = Draw.rdMolDraw2D.MolDraw2DCairo(512, 512)\n",
    "        d.drawOptions().rotate = 0\n",
    "        d.drawOptions().bondLineWidth = 1\n",
    "        d.DrawMolecule(Chem.MolFromSmiles(smi))\n",
    "        d.FinishDrawing()\n",
    "        idx = df[df['smiles'] == smi].index.to_numpy()[0]\n",
    "        d.WriteDrawingText(f\"{picFolder_path}/{idx}.jpeg\")\n",
    "\n",
    "elif type_of_image == 'spin_population':\n",
    "    sp_data = pd.read_csv(sp_path)\n",
    "    for i in range(len(sp_data)):\n",
    "        smi = sp_data.True_smiles.to_list()[i]\n",
    "        sp_list = sp_data.DensityArray.to_list()[i]\n",
    "        string_list = sp_list.strip('[]')\n",
    "        string_elements = string_list.split()\n",
    "        sp_list = [int(element) for element in string_elements]\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        densities = np.array(sp_list)\n",
    "        count = df[df['smiles'] == smi].index.to_numpy()[0]\n",
    "        fig = SimilarityMaps.GetSimilarityMapFromWeights(m,\n",
    "                                                         densities,\n",
    "                                                         colorMap='coolwarm',\n",
    "                                                         contourLines=10,\n",
    "                                                         colors='grey')\n",
    "        fig.savefig(f\"{picFolder_path}/{ring_class}_{substitution}/{count}.jpeg\",\n",
    "                    bbox_inches='tight',\n",
    "                    dpi=600)\n",
    "        fig.clear()"
   ],
   "id": "39664729669c264c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Automated analysis",
   "id": "d082d8a4b9878938"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have predicted the triplet energy and the spin density we can use an automated analysis to find the most promising candidates. For this we will use define the following criteria:\n",
    "1. The triplet energy is below a certain threshold (e.g. 62 kcal/mol)\n",
    "2. The spin density is located on the core of the molecule (e.g. thiophene)"
   ],
   "id": "2da8b87664f1113a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "threshold = 62 # Here we can define our triplet energy threshold\n",
    "\n",
    "path_to_et_datafile = home_dir + fr'Data/et_predictions/predictions_{ring_class}_{substitution}.csv'\n",
    "path_to_sp_datafile = home_dir + fr'Data/sp_predictions/predictions_{ring_class}_{substitution}.csv'\n",
    "path_to_save_candidates = home_dir + f'Data/potential_candidates_{ring_class}_{substitution}.csv'\n",
    "\n",
    "#--------------Load the data, and check if the maximum spin density is located on the core-------------------#\n",
    "data_et = pd.read_csv(path_to_et_datafile)\n",
    "data_sp = pd.read_csv(path_to_sp_datafile)\n",
    "\n",
    "data_sp = data_sp.rename(columns={'True_smiles': 'smiles'})\n",
    "data = data_sp.merge(data_et[['smiles', 'e_t']], on='smiles', how='left')\n",
    "\n",
    "data['DensityArray'] = data['DensityArray'].apply(lambda x: eval(x.replace(' ', ',')))\n",
    "\n",
    "density_located_on_core = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    predicted_smiles = row['Predicted_smiles']\n",
    "    density_array = data[data['smiles'] == predicted_smiles]['DensityArray'].values[0]\n",
    "\n",
    "    pattern_to_match = Chem.MolFromSmiles(smiles)\n",
    "    atom_index_of_core = list(Chem.MolFromSmiles(predicted_smiles).GetSubstructMatch(pattern_to_match))\n",
    "\n",
    "    density_on_core_list = []\n",
    "    density_on_substituent_list = []\n",
    "    for i in range(len(density_array)):\n",
    "        if i in atom_index_of_core:\n",
    "            density_on_core_list.append(density_array[i])\n",
    "        else:\n",
    "            density_on_substituent_list.append(density_array[i])\n",
    "\n",
    "    if max(density_on_core_list) >= max(density_on_substituent_list):\n",
    "        density_on_core=1\n",
    "    else:\n",
    "        density_on_core=0\n",
    "\n",
    "    density_located_on_core.append(density_on_core)\n",
    "\n",
    "data['Density_on_core'] = density_located_on_core\n",
    "\n",
    "\n",
    "#--------------Select the molecules that have a predicted triplet energy below the threshold and where the spin density is located on the ring-------------#\n",
    "\n",
    "interesting_data = data[data['e_t'] < threshold]\n",
    "interesting_data = interesting_data[interesting_data['Density_on_core'] == 1]\n",
    "interesting_data.to_csv(path_to_save_candidates)"
   ],
   "id": "2ea6bdb8a42210b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can have a look at the molecules that passed our criteria:",
   "id": "afff73587282c6dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "interesting_data",
   "id": "12e94f68530c132e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For a more convenient analysis we can interactively analyse the results.",
   "id": "372b64016f9a4e99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import os, dash, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from umap import UMAP\n",
    "from dash import dash_table\n",
    "import plotly.express as px\n",
    "from dash.dependencies import Input, Output\n",
    "from dash import html\n",
    "from dash import dcc\n",
    "\n",
    "#--------------Define which data shall be visualised and set the corresponding paths-------------------#\n",
    "\n",
    "representation = 'ECFP' # Available options are ECFP, MACCS, MACCS+ECFP\n",
    "\n",
    "picFolder_path = home_dir + f'Data/images/{ring_class}_{substitution}'\n",
    "data_path = home_dir + f'Data/virtual_libraries/{ring_class}_{substitution}.csv'\n",
    "e_t_path = home_dir + f'Data/et_predictions/predictions_{ring_class}_{substitution}.csv'\n",
    "\n",
    "#--------------Load the data, compute representation and reduce dimensionality-------------------#\n",
    "\n",
    "panda_dataframe = pd.read_csv(data_path)\n",
    "panda_dataframe = pd.concat([panda_dataframe, pd.read_csv(e_t_path, usecols =['e_t'])], axis=1)\n",
    "panda_dataframe['bins'] = pd.cut(panda_dataframe['e_t'], bins=np.arange(35,90,5), labels=np.arange(35,85,5))\n",
    "panda_dataframe['IDS'] = panda_dataframe.index.to_list()\n",
    "\n",
    "mol_list = [Chem.MolFromSmiles(smiles) for smiles in panda_dataframe['smiles']]\n",
    "\n",
    "if representation == 'ECFP':\n",
    "    fp_list = [AllChem.GetMorganFingerprintAsBitVect(molecule, radius=2, nBits=1024) for molecule in mol_list]\n",
    "if representation == 'MACCS':\n",
    "    fp_list = [MACCSkeys.GenMACCSKeys(molecule) for molecule in mol_list]\n",
    "if representation == 'MACCS+ECFP':\n",
    "    fp_list = [np.concatenate((MACCSkeys.GenMACCSKeys(molecule), AllChem.GetMorganFingerprintAsBitVect(molecule, radius=2, nBits=1024))) for molecule in mol_list]\n",
    "\n",
    "FP_array = np.array(fp_list)\n",
    "UMAPspace = UMAP(n_components=3,\n",
    "                 n_neighbors=60,\n",
    "                 min_dist=0.15,\n",
    "                 metric='euclidean',\n",
    "                 random_state=0,\n",
    "                 n_jobs=1\n",
    "                 ).fit(FP_array)\n",
    "\n",
    "panda_dataframe['UMAP_1'] = UMAPspace.embedding_[:, 0]\n",
    "panda_dataframe['UMAP_2'] = UMAPspace.embedding_[:, 1]\n",
    "panda_dataframe['UMAP_3'] = UMAPspace.embedding_[:, 2]\n",
    "data_array = panda_dataframe.to_numpy()\n",
    "\n",
    "#--------------Load the images for easy analysis of the data. Differentiate three data situations-------------------#\n",
    "\n",
    "globstatment = picFolder_path + '/*.jpeg'\n",
    "imageFiles = glob.glob(globstatment)\n",
    "\n",
    "if not imageFiles:                                                                                              # If no images are found in the folder, a dummy image is used\n",
    "    print(f\"No image files found in the folder: {picFolder_path} \\n Using dummy image instead.\")\n",
    "    default_image = home_dir + r'\\Data\\images\\default.png'\n",
    "    pics = {}\n",
    "    for id in panda_dataframe['IDS'].to_list():\n",
    "        encoded_image = base64.b64encode(open(default_image, 'rb').read()).decode(\"utf-8\")\n",
    "        pics[id] = encoded_image\n",
    "\n",
    "elif len(imageFiles) != len(data_array):                                                                        # If an inconsistend number of images is found (e.g, when the spin density map\n",
    "    pics = {}                                                                                                   # is not available for all molecules), a dummy image is used in these cases\n",
    "    print(f\"Number of images in folder {picFolder_path} ({len(imageFiles)} images) does not match the number \"\n",
    "          f\"of molecules in the dataset: {len(data_array)} \\nUsing dummy image for missing images instead.\")\n",
    "    for id in panda_dataframe['IDS'].to_list():\n",
    "        try:\n",
    "            img_ = picFolder_path + f'/{id}.jpeg'\n",
    "            encoded_image = base64.b64encode(open(img_, 'rb').read()).decode(\"utf-8\")\n",
    "        except:\n",
    "            default_image = home_dir + r'\\Data\\images\\default.png'\n",
    "            encoded_image = base64.b64encode(open(default_image, 'rb').read()).decode(\"utf-8\")\n",
    "        pics[id] = encoded_image\n",
    "\n",
    "else:                                                                                                            # If the number of images matches the number of molecules, the images are loaded normally\n",
    "    pics = {}\n",
    "    for i, file in enumerate(imageFiles):\n",
    "        encoded_image = base64.b64encode(open(file, 'rb').read()).decode(\"utf-8\")\n",
    "        file_name = os.path.basename(file.split('.')[0])\n",
    "        id = int(file_name)\n",
    "        pics[id] = encoded_image\n",
    "\n",
    "def generate_info_dataframe(mol_id):\n",
    "    '''\n",
    "    Function required for the interactive analysis. It generates a pandas dataframe containing\n",
    "    the information about a molecule that is displayed in the table.\n",
    "    '''\n",
    "    index = int(np.where(data_transpose[:, 3] == mol_id)[0])\n",
    "    mol_dataPoint = data_transpose[index]\n",
    "    mol_SMILES = mol_dataPoint[0]\n",
    "    mol_e_t = np.round(mol_dataPoint[1],2)\n",
    "\n",
    "    dict = {}\n",
    "    dict['Descriptors'] = ['ID', 'Smiles', 'Triplet energy [kcal/mol]']\n",
    "    dict['Value'] = [mol_id, mol_SMILES, mol_e_t]\n",
    "    return pd.DataFrame(dict)\n",
    "\n",
    "#--------------Prepare the 3D plot-------------------#\n",
    "\n",
    "data, data_transpose, mol_pics = data_array.transpose(), data_array, pics\n",
    "plot_df = pd.DataFrame(data)\n",
    "df = generate_info_dataframe(data_transpose[0][3])\n",
    "\n",
    "# initiating the app\n",
    "app = dash.Dash()  # defining the layout\n",
    "\n",
    "fig = px.scatter_3d(x=data[4],\n",
    "                    y=data[5],\n",
    "                    z=data[6],\n",
    "                    hover_name=data[3],\n",
    "                    color=data[2],\n",
    "                    color_discrete_map = {  35: '#e5f5e0',  # light green\n",
    "                                            40: '#a1d99b',  # soft green\n",
    "                                            45: '#74c476',  # medium green\n",
    "                                            50: '#31a354',  # dark green\n",
    "                                            55: '#006d2c',  # very dark green\n",
    "                                            60: '#fcbba1',  # light red\n",
    "                                            65: '#fc9272',  # medium red\n",
    "                                            70: '#de2d26'},  # dark red\n",
    "                    labels={'x': 'UMAP_1',\n",
    "                            'y': 'UMAP_2',\n",
    "                            'z': 'UMAP_3'},\n",
    "                    height=900,\n",
    "                    width=950)\n",
    "\n",
    "fig.update_traces(marker=dict(size=4,\n",
    "                              opacity=0.7,\n",
    "                              line=dict(width=1,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "app.layout = html.Div([\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Graph(id='DB-SMILES',\n",
    "                  figure=fig\n",
    "                  )],\n",
    "        style={'width': '60%', 'display': 'inline-block', 'border-style': 'solid', 'vertical-align': 'middle'}),\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.Img(id='mol_pic', src=f'data:image/png;base64,{mol_pics[0]}')\n",
    "        ], id='mol_pic_div'),\n",
    "        html.Div([\n",
    "            dash_table.DataTable(\n",
    "                id='mol_infotable',\n",
    "                columns=[{\"name\": i, \"id\": i} for i in df.columns],\n",
    "                data=df.to_dict('records')\n",
    "            )], id='mol_infotable_div')], style={'width': '30%', 'display': 'inline-block',\n",
    "                                                 'margin-left': '50px', 'border-style': 'solid',\n",
    "                                                 'vertical-align': 'middle'})\n",
    "])\n",
    "\n",
    "@app.callback(Output('mol_infotable_div', 'children'), [Input('DB-SMILES', 'hoverData')])\n",
    "def callback_stats(hoverData):\n",
    "    if hoverData is None:\n",
    "        mol_id = 0\n",
    "    else:\n",
    "        point = hoverData['points'][0]\n",
    "        mol_id = int(point['hovertext'])\n",
    "\n",
    "    out_infos = generate_info_dataframe(mol_id)\n",
    "\n",
    "    dt = dash_table.DataTable(\n",
    "        id='mol_infotable',\n",
    "        columns=[{\"name\": i, \"id\": i} for i in out_infos.columns],\n",
    "        data=out_infos.to_dict('records'),\n",
    "        style_cell_conditional=[\n",
    "            {'if': {'column_id': 'Descriptors'},\n",
    "             'width': '30%'},\n",
    "            {'if': {'column_id': 'Value'},\n",
    "             'width': '70%'},\n",
    "        ],\n",
    "        style_data={\n",
    "            'overflow': 'hidden',\n",
    "            'textOverflow': 'ellipsis',\n",
    "            'maxWidth': 0,\n",
    "            'whiteSpace': 'normal',\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return dt\n",
    "\n",
    "@app.callback(Output('mol_pic_div', 'children'), [Input('DB-SMILES', 'hoverData')])\n",
    "def callback_stats(hoverData):\n",
    "    if hoverData is None:\n",
    "        mol_id = int(0)\n",
    "        return html.Img(\n",
    "            id='mol_pic',\n",
    "            src=f'data:image/png;base64,{mol_pics[mol_id]}',\n",
    "            style={'width': '85%', 'height': 'auto', 'display': 'block', 'margin': 'auto'}\n",
    "        )\n",
    "    else:\n",
    "        point = hoverData['points'][0]\n",
    "        mol_id = int(point['hovertext'])\n",
    "        return html.Img(\n",
    "            id='mol_pic',\n",
    "            src=f'data:image/png;base64,{mol_pics[mol_id]}',\n",
    "            style={'width': '85%', 'height': 'auto', 'display': 'block', 'margin': 'auto'}\n",
    "        )\n",
    "\n",
    "app.run_server(mode='external')"
   ],
   "id": "1ac5da84305d6383",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
